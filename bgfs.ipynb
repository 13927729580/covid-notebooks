{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bgfs.ipynb\n",
    "\n",
    "Fit various functions to COVID-19 time series for U.S. counties using the Broyden–Fletcher–Goldfarb–Shanno solver from `sklearn.optimize`.\n",
    "\n",
    "Inputs:\n",
    "* `data/us_counties_clean.csv`: The contents of `data/us_counties.csv` after data cleaning by `clean.ipynb`\n",
    "* `data/us_counties_clean_meta.json`: Column type metadata for reading `data/us_counties_clean.csv` with `pd.read_csv()`\n",
    "\n",
    "Outputs:\n",
    "* `data/us_counties_curves.csv`: The curves that this notebook generated\n",
    "* `data/us_counties_curves_meta.json`: Column type metadata for reading `data/us_counties_curves.csv` with `pd.read_csv()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization boilerplate\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from sklearn import metrics\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import text_extensions_for_pandas as tp\n",
    "\n",
    "# Local file of utility functions\n",
    "import util\n",
    "\n",
    "# What precision of floating-point to use.\n",
    "# Consider 32-bit if using GPU-accelerated solvers. Otherwise, 64-bit\n",
    "# floating point is better because it reduces the chance of divergence.\n",
    "fp_type = np.float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Population</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Confirmed_Outlier</th>\n",
       "      <th>Deaths_Outlier</th>\n",
       "      <th>Recovered_Outlier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1001</th>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">56045</th>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289064 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    State   County  Population  Confirmed  Deaths  Recovered  \\\n",
       "FIPS  Date                                                                     \n",
       "1001  2020-01-22  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-23  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-24  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-25  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-26  Alabama  Autauga       55869          0       0          0   \n",
       "...                   ...      ...         ...        ...     ...        ...   \n",
       "56045 2020-04-18  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-19  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-20  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-21  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-22  Wyoming   Weston        6927          0       0          0   \n",
       "\n",
       "                  Confirmed_Outlier  Deaths_Outlier  Recovered_Outlier  \n",
       "FIPS  Date                                                              \n",
       "1001  2020-01-22              False           False              False  \n",
       "      2020-01-23              False           False              False  \n",
       "      2020-01-24              False           False              False  \n",
       "      2020-01-25              False           False              False  \n",
       "      2020-01-26              False           False              False  \n",
       "...                             ...             ...                ...  \n",
       "56045 2020-04-18              False           False              False  \n",
       "      2020-04-19              False           False              False  \n",
       "      2020-04-20              False           False              False  \n",
       "      2020-04-21              False           False              False  \n",
       "      2020-04-22              False           False              False  \n",
       "\n",
       "[289064 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the CSV file and apply the saved type information\n",
    "csv_file = \"data/us_counties_clean.csv\"\n",
    "meta_file = \"data/us_counties_clean_meta.json\"\n",
    "\n",
    "# Read column type metadata\n",
    "with open(meta_file) as f:\n",
    "    cases_meta = json.load(f)\n",
    "\n",
    "# Pandas does not currently support parsing datetime64 from CSV files.\n",
    "# As a workaround, read the \"Date\" column as objects and manually \n",
    "# convert after.\n",
    "cases_meta[\"Date\"] = \"object\"\n",
    "\n",
    "cases_vertical = (\n",
    "    pd\n",
    "    .read_csv(csv_file, dtype=cases_meta, parse_dates=[\"Date\"])   \n",
    "    .set_index([\"FIPS\", \"Date\"], verify_integrity=True)\n",
    ")\n",
    "cases_vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Population</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Confirmed_Outlier</th>\n",
       "      <th>Deaths_Outlier</th>\n",
       "      <th>Recovered_Outlier</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIPS</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1001</th>\n",
       "      <th>2020-01-22</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-23</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-24</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-25</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-26</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>55869</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">56045</th>\n",
       "      <th>2020-04-18</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-19</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-20</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Weston</td>\n",
       "      <td>6927</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289064 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    State   County  Population  Confirmed  Deaths  Recovered  \\\n",
       "FIPS  Date                                                                     \n",
       "1001  2020-01-22  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-23  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-24  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-25  Alabama  Autauga       55869          0       0          0   \n",
       "      2020-01-26  Alabama  Autauga       55869          0       0          0   \n",
       "...                   ...      ...         ...        ...     ...        ...   \n",
       "56045 2020-04-18  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-19  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-20  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-21  Wyoming   Weston        6927          0       0          0   \n",
       "      2020-04-22  Wyoming   Weston        6927          0       0          0   \n",
       "\n",
       "                  Confirmed_Outlier  Deaths_Outlier  Recovered_Outlier  \n",
       "FIPS  Date                                                              \n",
       "1001  2020-01-22                  0               0                  0  \n",
       "      2020-01-23                  0               0                  0  \n",
       "      2020-01-24                  0               0                  0  \n",
       "      2020-01-25                  0               0                  0  \n",
       "      2020-01-26                  0               0                  0  \n",
       "...                             ...             ...                ...  \n",
       "56045 2020-04-18                  0               0                  0  \n",
       "      2020-04-19                  0               0                  0  \n",
       "      2020-04-20                  0               0                  0  \n",
       "      2020-04-21                  0               0                  0  \n",
       "      2020-04-22                  0               0                  0  \n",
       "\n",
       "[289064 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As a workaround for a bug in Pandas' extension types system,\n",
    "# we need to cast the boolean columns to ints.\n",
    "for col in [\"Confirmed_Outlier\", \"Deaths_Outlier\", \"Recovered_Outlier\"]:\n",
    "    cases_vertical[col] = cases_vertical[col].astype(np.int8)\n",
    "cases_vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse each time series or mask down to a single cell\n",
    "cases, dates = util.collapse_time_series(cases_vertical, [\n",
    "    \"Confirmed\", \"Deaths\", \"Recovered\", \n",
    "    \"Confirmed_Outlier\", \"Deaths_Outlier\", \"Recovered_Outlier\"])\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify the code that follows, we only fit curves to one of\n",
    "# the time series for each county. \n",
    "# Change the following constant to use a different time series:\n",
    "ts_col_name = \"Confirmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because these time series are integer-valued, there can be\n",
    "# problems with aliasing. \n",
    "# Here are some examples of what aliasing looks like:\n",
    "# graph_examples() function defined in util.py):\n",
    "util.graph_examples(cases, ts_col_name, {}, \n",
    "                    mask=(np.max(cases[ts_col_name].values, axis=1) < 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid problems due to aliasing, we will restrict the analysis\n",
    "# in the rest of this notebook to time series whose maximum values\n",
    "# go above a threshold:\n",
    "alias_threshold = 100\n",
    "\n",
    "# We also cut off the sections at the beginning of the time \n",
    "# series where every time series' value is below this threshold.\n",
    "\n",
    "# Find what point in the time series at least one county went above\n",
    "# the threshold.\n",
    "first_time_above_min = np.argmax(np.max(cases[ts_col_name].values, axis=0) >= alias_threshold)\n",
    "print(f\"Dropping the first {first_time_above_min} elements of each time series.\")\n",
    "\n",
    "# Find which counties have at least one time series value above the \n",
    "# threshold.\n",
    "counties_mask = np.max(cases[ts_col_name].values, axis=1) >= alias_threshold\n",
    "\n",
    "# Filter rows\n",
    "filtered = cases[counties_mask].copy(deep=True)\n",
    "\n",
    "# Truncate time series to just the times when at least one county\n",
    "# was above our threshold.\n",
    "filtered[ts_col_name] = filtered[ts_col_name].values[:,first_time_above_min:]\n",
    "\n",
    "# Also filter the outlier masks\n",
    "outlier_col_name = ts_col_name + \"_Outlier\"\n",
    "filtered[outlier_col_name] = filtered[outlier_col_name].values[:,first_time_above_min:]\n",
    "\n",
    "filtered_dates = dates[first_time_above_min:]\n",
    "\n",
    "# Drop time series columns other than the one we analyze\n",
    "series_to_keep = [ts_col_name, outlier_col_name]\n",
    "metadata_cols = []\n",
    "to_drop = []\n",
    "for colname in filtered.columns:\n",
    "    if not isinstance(filtered[colname].dtype, tp.TensorType):\n",
    "        metadata_cols.append(colname)\n",
    "    elif colname not in series_to_keep:\n",
    "        to_drop.append(colname)\n",
    "        \n",
    "filtered = filtered.drop(columns=to_drop)\n",
    "\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-up: Fit a straight line.\n",
    "# We use sklearn's implementation of BGFS as the optimizer.\n",
    "\n",
    "# What precision of floating-point to use\n",
    "# On a laptop, there's not much reason to use 32-bit.\n",
    "fp_type = np.float64\n",
    "series_values = filtered[ts_col_name].values.astype(fp_type)\n",
    "series_len = series_values.shape[1]\n",
    "x = np.linspace(0, series_len - 1, series_len, dtype=fp_type)\n",
    "\n",
    "def linear_curve(m, b):\n",
    "    return m * x + b\n",
    "\n",
    "def linear_objective(var_values, y):\n",
    "    m, b = var_values\n",
    "    return np.sum((y - (linear_curve(m, b))) ** 2)\n",
    "\n",
    "linear_initial_guess = (0.1, 1.0)\n",
    "linear_bounds = ((0.0, 1000.0), (float(-series_len), float(series_len)))\n",
    "\n",
    "linear = filtered[metadata_cols].copy()\n",
    "linear[\"Result_Object\"] = [\n",
    "    scipy.optimize.minimize(\n",
    "        linear_objective, linear_initial_guess,\n",
    "        args=(ts), bounds=linear_bounds)\n",
    "    for ts in series_values\n",
    "]\n",
    "linear[\"Slope\"] = linear[\"Result_Object\"].apply(lambda r: r.x[0])\n",
    "linear[\"Intercept\"] = linear[\"Result_Object\"].apply(lambda r: r.x[1])\n",
    "\n",
    "# Generate all the linear curves\n",
    "M = linear[\"Slope\"].values.reshape([-1, 1])\n",
    "X = x.reshape([1, -1])\n",
    "B = linear[\"Intercept\"].values.reshape([-1, 1])\n",
    "linear[\"Curve\"] = tp.TensorArray(M * X + B)\n",
    "\n",
    "# Compute coefficient of determination\n",
    "linear[\"R^2\"] = [\n",
    "    metrics.r2_score(\n",
    "        filtered.loc[fips][ts_col_name], linear.loc[fips][\"Curve\"]) \n",
    "        for fips in filtered.index]\n",
    "\n",
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lines we just fit, using the graph_examples() function in util.py.\n",
    "util.graph_examples(filtered, ts_col_name, {\"Linear\": linear})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's fit some more appropriate curves to this data.\n",
    "# First, break out the repetitive parts of the curve-fitting process into\n",
    "# a Python function.\n",
    "def curve_fit_df(df: pd.DataFrame, ts_col_name: str,\n",
    "                 curve_fn,\n",
    "                 param_names: Tuple,\n",
    "                 initial_guess: Tuple,\n",
    "                 bounds: Tuple,     \n",
    "                 regularize_fn = None):\n",
    "    \"\"\"\n",
    "    Fit a curve to a column full of time series.\n",
    "    \n",
    "    :param df: DataFrame containing one time series per cell\n",
    "    :param ts_col_name: Column containing the time series to analyze\n",
    "    :param curve_fn: Function that, if called with a vector of X values\n",
    "     and a tuple of parameters, will compute the value of the curve \n",
    "     being fit.\n",
    "     If parameters are vectors, this function must compute multiple\n",
    "     curves, one for each element of the vectors.\n",
    "    \n",
    "    :param param_names: Names of the parameters of `curve_fn`.\n",
    "    :param initial_guess: Initial values for the paramters to start\n",
    "     off the solver.\n",
    "    :param bounds: Lower and upper bounds for the parameters.\n",
    "    :param regularize_fn: Optional function that returns a \n",
    "     regularization penalty, given a set of parameter values.\n",
    "    \n",
    "    :returns: A dataframe with one row per row of `df`, containing\n",
    "     information about the \n",
    "    \"\"\"\n",
    "    series_values = df[ts_col_name].values.astype(fp_type)\n",
    "    series_len = series_values.shape[1]\n",
    "    x = np.linspace(0, series_len - 1, series_len, dtype=fp_type)\n",
    "    \n",
    "    def return_zero(var_values):\n",
    "        return 0.0\n",
    "    \n",
    "    if regularize_fn is None:\n",
    "        regularize_fn = return_zero\n",
    "    \n",
    "    def objective(var_values, y):\n",
    "        squared_diffs = np.sum((y - (curve_fn(x, var_values))) ** 2)\n",
    "        return squared_diffs + regularize_fn(var_values)\n",
    "    \n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    result[metadata_cols] = df[metadata_cols]\n",
    "    result[\"Result_Object\"] = [\n",
    "        scipy.optimize.minimize(\n",
    "            objective, initial_guess,\n",
    "            args=(ts), bounds=bounds)\n",
    "        for ts in series_values\n",
    "    ]\n",
    "    \n",
    "    # Add a column for each parameter\n",
    "    for i in range(len(param_names)):\n",
    "        name = param_names[i]\n",
    "        result[name] = result[\"Result_Object\"].apply(lambda r: r.x[i])\n",
    "\n",
    "    # Generate all the curves\n",
    "    param_vectors = [\n",
    "        result[name].values.reshape([-1, 1])\n",
    "        for name in param_names\n",
    "    ]\n",
    "    result[\"Curve\"] = tp.TensorArray(curve_fn(x, param_vectors))\n",
    "\n",
    "    # Compute coefficient of determination\n",
    "    result[\"R^2\"] = [\n",
    "        metrics.r2_score(\n",
    "            df.loc[fips][ts_col_name], result.loc[fips][\"Curve\"]) \n",
    "            for fips in df.index]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define curve-fitting routines using the function from the previous\n",
    "# cell.\n",
    "\n",
    "def fit_exponential(ts_col_name: str):\n",
    "    \"\"\"\n",
    "    Fit an exponential curve to each time series in the specified column\n",
    "    of the `cases` dataframe.\n",
    "    \"\"\"    \n",
    "    def curve_fn(x, var_values):\n",
    "        rate, offset = var_values\n",
    "        # Y = e^(rate)(X - offset)\n",
    "        return np.exp(rate * (x - offset))\n",
    "    \n",
    "    param_names = (\"Rate\", \"Offset\")\n",
    "    initial_guess = (0.1, 1.0)\n",
    "    bounds = ((0.0, 1.0), (0.0, float(len(dates))))\n",
    "    \n",
    "    return curve_fit_df(filtered, ts_col_name, curve_fn,\n",
    "                        param_names, initial_guess, bounds)\n",
    "\n",
    "\n",
    "def fit_logistic(ts_col_name: str):\n",
    "    \"\"\"\n",
    "    Fit a logistic curve to each time series in the specified column\n",
    "    of the `cases` dataframe.\n",
    "    \"\"\"    \n",
    "    def curve_fn(x, var_values):\n",
    "        max_, rate, offset = var_values\n",
    "        # Y = max / (1 + e^(-rate *(X - offset))\n",
    "        return max_ / (1.0 + np.exp(-rate * (x - offset)))\n",
    "    \n",
    "    param_names = (\"Max\", \"Rate\", \"Offset\")\n",
    "    initial_guess = (1000.0, 0.1, 1.0)\n",
    "    bounds = ((0.0, 1e6), (0.0, 1.0), (0.0, float(len(dates))))\n",
    "    \n",
    "    return curve_fit_df(filtered, ts_col_name, curve_fn,\n",
    "                        param_names, initial_guess, bounds)\n",
    "\n",
    "\n",
    "def fit_logistic2(ts_col_name: str):\n",
    "    \"\"\"\n",
    "    Fit a mixture of two logistic curve sto each time series in the \n",
    "    specified column of the `cases` dataframe.\n",
    "    \"\"\"\n",
    "    # Distance from X=0 at which the logistic function is close enough to 0 or 1\n",
    "    # that we can consider it to have \"triggered\"\n",
    "    logistic2_limit = np.array(6., dtype=fp_type)\n",
    "    \n",
    "    def curve_fn(x, var_values):\n",
    "        (max1, rate1, offset1, \n",
    "         max2, rate2, offset2, \n",
    "         switch_begin, switch_end) = var_values\n",
    "        \n",
    "        # logistic curve is Y = max / (1 + e^(-rate *(X - offset))\n",
    "        logistic1 = max1 / (1.0 + np.exp(-rate1 * (x - offset1)))\n",
    "        logistic2 = max2 / (1.0 + np.exp(-rate2 * (x - offset2)))\n",
    "\n",
    "        # What fraction of the output comes from logistic2?\n",
    "        def sigmoid(x_):\n",
    "            return 1. / (1. + np.exp(-x_))\n",
    "        \n",
    "        mix_input = (x - switch_begin - logistic2_limit) / np.maximum(1.0, (switch_end - switch_begin))\n",
    "        return logistic1 * (1.0 - sigmoid(mix_input)) + logistic2 * sigmoid(mix_input)\n",
    "    \n",
    "    def regularize_fn(var_values):\n",
    "        (max1, rate1, offset1, \n",
    "         max2, rate2, offset2, \n",
    "         switch_begin, switch_end) = var_values\n",
    "        reg_coeff = 1e7\n",
    "        linear_terms = [\n",
    "            max(0.0, switch_begin - switch_end - 1.0)  # switch_end >= switch_begin + 1\n",
    "        ]\n",
    "        return reg_coeff * sum([t for t in linear_terms])\n",
    "    \n",
    "    param_names = (\n",
    "        \"Max1\", \"Rate1\", \"Offset1\",\n",
    "        \"Max2\", \"Rate2\", \"Offset2\",\n",
    "        \"Switch_Begin\", \"Switch_End\")\n",
    "    initial_guess = (\n",
    "        1000.0, 0.1, 1.0,  # First logistic function\n",
    "        1000.0, 0.1, 1.0,  # Second logistic function\n",
    "        10.0, 20.0  # Switchover\n",
    "    )\n",
    "    series_len = len(dates)\n",
    "    bounds = (\n",
    "        (0.0, 1e6), (0.0, 10.0), (0.0, float(series_len)),   # First logistic function\n",
    "        (0.0, 1e6), (0.0, 10.0), (0.0, float(series_len)),   # Second logistic function\n",
    "        (0.0, float(series_len)), (0.0, float(series_len))  # Switchover\n",
    "    )\n",
    "    \n",
    "    return curve_fit_df(filtered, ts_col_name, curve_fn,\n",
    "                        param_names, initial_guess, bounds,\n",
    "                        regularize_fn=regularize_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit those curves.\n",
    "# First, the exponential curves.\n",
    "exp_df = fit_exponential(ts_col_name)\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.graph_examples(filtered, ts_col_name, {\"Exponential\": exp_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then fit the logistic function\n",
    "log_df = fit_logistic(ts_col_name)\n",
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.graph_examples(filtered, ts_col_name, {\"Logistic\": log_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally a mixture of two copies of the logistic function\n",
    "log2_df = fit_logistic2(ts_col_name)\n",
    "log2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.graph_examples(filtered, ts_col_name, {\"Logistic2\": log2_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stuff all the curves that we have fit into a single dataframe,\n",
    "# along with the original series' values\n",
    "curves_df = filtered[metadata_cols + [ts_col_name, outlier_col_name]].copy()\n",
    "curves_df[\"Exponential\"] = exp_df[\"Curve\"]\n",
    "curves_df[\"Logistic\"] = log_df[\"Curve\"]\n",
    "curves_df[\"Logistic2\"] = log2_df[\"Curve\"]\n",
    "curves_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand out the tensors to multiple rows for writing to a CSV file, and\n",
    "# cast the boolean values back to np.bool\n",
    "curves_vertical = util.explode_time_series(curves_df, filtered_dates)\n",
    "curves_vertical[\"Confirmed_Outlier\"] = curves_vertical[\"Confirmed_Outlier\"].astype(np.bool)\n",
    "curves_vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the results to a CSV file plus a JSON file of type metadata.\n",
    "curves_vertical.to_csv(\"data/us_counties_curves.csv\", index=True)\n",
    "col_type_mapping = {\n",
    "    key: str(value) for key, value in curves_vertical.dtypes.iteritems()\n",
    "}\n",
    "with open(\"data/us_counties_curves_meta.json\", \"w\") as f:\n",
    "    json.dump(col_type_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh data/us_counties_curves*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
